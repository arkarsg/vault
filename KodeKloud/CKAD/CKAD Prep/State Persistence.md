# Storage in Docker

There are 2 concepts: **Storage drivers** and **volume drivers**
- Volumes are not handled by storage drivers
- Volumes are handled by volume driver plugins


## Storage drivers

>[!note] Storage with containers
>- Docker stores all data in `/var/lib/docker`: files related to images and files related to containers.
>- Any volumes created by Docker will be stored in the volumes folder
>
>Suppose a Docker image is created with $n$ layers. These layers are *read only*. When `docker run` is executed, files created/ modified by the docker image such as logs exists in the **container layer** which is *read-write*. However, the files only exists as long as the container is alive.
>
>**What if I wish to modify the image?**
>- You can still modify the file. Docker creates a copy in the read-write layer with Copy-On-Write mechanism. However, it will not persist beyond the life of the container.
>
>To persist, add a persistent volume to the container
>1. Create persistent volume
>```bash
># create data_volume in /var/lib/docker/volumes/
>docker volume create data_volume
>```
>2. Mount volume inside docker containers 
>```bash
>docker run -v data_volume:/var/lib/mysql mysql
>```
>
>![docker-volume|500](Screenshot%202024-06-21%20at%204.01.36%20PM.png)
>
>All data written to `/var/lib/mysql` is in fact written to `data_volume` on the Docker host. Even if the container is destroyed, data is still persisted.
>
>```bash
># running without creating a volume creates a volume automatically
>docker run -v data_volume2:/var/lib/mysql mysql
>```
>
>**What if there is existing data somewhere?**
>- Provide the complete path
>```bash
>docker run -v /data/mysql:/var/lib/mysql mysql
>```
>This is called **bind mounting** which mounts from any directory from Docker host.
>
>**How does Docker move files, CoW mechanism?**
>Docker uses **storage drivers**
>- AUFS
>- ZFS
>- BTRFS
>- Device mapper
>- Overlay
>- Overlay2
>  
> Docker chooses the best storage driver based on the operating systems. This can cause differences in performance etc.

>[!caution]
>Instead of `-v`, `--mount` is preferred:
>```bash
>docker run \
>–mount type=bind,source=/data/mysql,target=/var/lib/mysql
>mysql
>```

## Volume drivers
Default volume drivers: Local
- Store its data under `/var/lib/docker/volumes` directory

Volume drivers allow you to store on third party storage providers

```bash
docker run -it \
	--name mysql
	--volume-driver rexray/ebs
	--mount src=ebs-vol,target=/var/lib/mysql
	mysql
```

This will create a container and attach a volume from AWS EBS.

---

# Storage in Kubernetes

## Volumes
Pods created in Kubernetes are transient in nature. Data processed by pod is transient as well. To persist data beyond the lifecycle of a pod,
- Attach a volume to the pod
- Data generated by the pod is now stored in the volume

### Example
Suppose a pod generates a random number and writes to a file:
```yaml
apiVersion: v1
kind: Pod
metadata:
	name: random-number-generator
spec:
	volumes:
	- name: data-volume
	  hostPath:
		  path: /data
		  type: Directory
	containers:
	- image: alpine
	  name: alpine
	  command: ["/bin/sh", "-c"]
	  args: ["shuf -i 0-100 -n 1 >> /opt/number.out"]
	  volumeMounts:
	  - mountPath: /opt
	    name: data-volume
```

`volumes` creates a volume. Any file written to the volume is written to the `/data` folder in the node.

`volumeMounts` field in each container mount the volume to the mount path

>[!caution]
>Specifying the `hostPath` is not recommended for multi-node cluster. This is because the pods will expect all the nodes to have `/data` directory and have the same data.
>
>Since they are on different servers, they are in fact not the same.

#### Using AWS EBS as volume

```yaml
volumes:
- name: data-volume
  awsElasticBlockStore:
	  volumeId: <volume-id>
	  fsType: ext4
```

---

## Persistent volumes
With volumes, user who deploy the pods would have to configure that on all pod definition files in his environment.

**How to manage storage centrally?**
- Create a large pool of storage and have users carve out storage as required

>[!note] Persistent volume
>A cluster wide pool of storage volumes configured by an administrator to be used by users deploying applications on the cluster
>
>Users can now select storage from the pool using *persistent volume claim*

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
	name: pv-vol1
spec:
	accessModes:
		- ReadWriteOnce
	capacity:
		storage: 1 Gi
	hostPath:
		path: /tmp/data
```

```bash
k create -f pv-definition.yaml
```

>[!caution]
>Replace host path with one of the supported storage options

---

## Persistent Volume Claims

Make storage available to a node.

User creates *persistent volume claims* to use the storage.

Every PVC is bound to a single PV. Kubernetes tries to find a PV that has sufficient capacity. If there are multiple matches, labels can be used to bind to the right PV.

There is a one-to-one claim between PV and PVC

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
	name: my-claim
spec:
	accessModes:
		- ReadWriteOnce
	resources:
		requests:
			storage: 500Mi
```

```bash
k create -f my-claim.yaml
```

When a claim is deleted, PV is retained by default. 
- Retained
- Delete
- Recycle

---

Once you create a PVC use it in a POD definition file by specifying the PVC Claim name under `persistentVolumeClaim` section in the volumes section like this:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
    - name: myfrontend
      image: nginx
      volumeMounts:
      - mountPath: "/var/www/html"
        name: mypd
  volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: myclaim
```

The same is true for ReplicaSets or Deployments. Add this to the pod template section of a Deployment on ReplicaSet.

---

# Storage Classes

## Dynamic Provisioning
Define a provisioner such as Google Storage that can automatically provision storage on Google Cloud and attach to a Pod when a claim is made

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
	name: google-storage
provisioner: kubernetes.io/gce-p4
```

We no longer need the PV definition. PV is created automatically with storage class.

In PVC, define the storage class to use.

Storage Class uses the defined provisioner to provision a new disk with the required size on GCP. Then, it creates a PVC and binds to that volume.

>[!note] Other provisioners…
>- AWS EBS
>- Azure File
>- Azure Disk

You can also specify the type of disk, replication mode, depending on the provider

---

# StatefulSets

>[!note] Why do we need StatefulSets?
>Suppose we want to deploy a database cluster with replications: There are master host and slave hosts.
>
>In Kubernetes deployments, all pods come up at the same time and cannot guarantee that master comes up first and slaves come up later.
>
>Furthermore, we need a constant IP address of the master host for the slaves.
>
>If a master host fails, slave pods will point to a pod that no longer exists/ wrong name


StatefulSets creates pods in a sequential order. After a pod is deployed, it must be in a ready/running state before the next pod is deployed.

Therefore, we can deploy master and slave sequentially.

Each pod is given an index and its name is derived from the index. Therefore, there are no more random names. 
- The first pod will always be the name of the pod followed by `-0`

Even if the master fails and recreates the pod, it will have the same name. StatefulSets have sticky identity.

---

>[!caution] You might not always need a StatefulSet
>Need stateful sets when
>- Pods need to come up in a certain order
>- Pods need a consistent name

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
	name: mysql
	labels:
		app: mysql
spec:
	template:
		metadata:
			labels:
				app: mysql
		spec:
			containers:
			- name: mysql
			  image: mysql
	replicas: 3
	selector:
		matchLabels:
			app: mysql
	serviceName: mysql-h
	# tells StatefulSet that order does not matter
	# podManagementPolicy: parallel 
```

---

## Headless Services
When a StatefulSet is created, pods are deployed one at a time with a stable index. Then, pods can point to one another.

However, we know that pods typically communicate through `Services`.

The service has a ClusterIP and a DNS name associated with it. Any other application such as web server can reach the DB through DNS.

How to ensure that reads are from MySQL service but limit write to master database only?

![headless-service|500](Screenshot%202024-06-21%20at%205.34.17%20PM.png)

We need a Service that does not load-balance requests but gives us a DNS entry to reach each pod.

A Headless Service does not have an IP address on its own. It just creates a DNS entry with DNS record for each pod.

The web application can now point to the master database on the headless service.

```yaml
apiVersion: v1
kind: Service
metadata:
	name: mysql-h
spec:
	ports:
	- port: 3306
	selector:
		app: mysql
	clusterIP: None
```

To create a pod that uses the headless service,
```yaml
apiVersion: v1
kind: Pod
metadata:
	name: myapp-pod
	labels:
		app: mysql
spec:
	containers:
	- name: mysql
	  image: mysql
	subdomain: mysql-h # the headless service
	hostname: mysql-pod # to create A records for inidividual pods
```

With Deployments, this will still not allow users to reach the pods.

With StatefulSets, `subdomain` or `hostname` does not have to be specified. `hostname` is specified based on the pod name and automatically assigns the `subdomain` based on the headless service which is explicitly defined.

All pods now get a separate DNS record created.

---

## Storage in StatefulSets
With StatefulSet, when a PVC is defined, it will use the same volumes. However, not all provisioners allow read and write concurrently.

What if each pod needs a PVC for itself?

VolumeClaimTemplate templatizes PVC.

Suppose we have a stateful set with Volume claim and SC.

The first pod of stateful set creates a PVC. PVC is associated to SC so PV is created and associated to a volume. PV is bound to PVC.

If a pod fails, PVC and associated volume is not deleted. It ensures that these are attached to the same pod. This ensures that there is stable storage.


